{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import to_categorical\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dense, Dropout, Flatten, BatchNormalization\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import load_model\n",
    "from keras.optimizers import RMSprop ,Adadelta ,SGD ,Adamax"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the CSV file and the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>InstanceID</th>\n",
       "      <th>patientID</th>\n",
       "      <th>ImageName</th>\n",
       "      <th>cellTypeName</th>\n",
       "      <th>cellType</th>\n",
       "      <th>isCancerous</th>\n",
       "      <th>image_data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22405</td>\n",
       "      <td>1</td>\n",
       "      <td>22405.png</td>\n",
       "      <td>fibroblast</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[[[242, 213, 237], [239, 210, 234], [236, 206,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>22406</td>\n",
       "      <td>1</td>\n",
       "      <td>22406.png</td>\n",
       "      <td>fibroblast</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[[[221, 175, 211], [223, 175, 211], [229, 180,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>22407</td>\n",
       "      <td>1</td>\n",
       "      <td>22407.png</td>\n",
       "      <td>fibroblast</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[[[247, 243, 247], [248, 242, 246], [247, 240,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>22408</td>\n",
       "      <td>1</td>\n",
       "      <td>22408.png</td>\n",
       "      <td>fibroblast</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[[[243, 242, 242], [243, 241, 241], [243, 241,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>22409</td>\n",
       "      <td>1</td>\n",
       "      <td>22409.png</td>\n",
       "      <td>fibroblast</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[[[189, 130, 189], [187, 127, 185], [187, 126,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9891</th>\n",
       "      <td>1625</td>\n",
       "      <td>60</td>\n",
       "      <td>1625.png</td>\n",
       "      <td>epithelial</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>[[[171, 138, 180], [206, 175, 217], [238, 209,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9892</th>\n",
       "      <td>1626</td>\n",
       "      <td>60</td>\n",
       "      <td>1626.png</td>\n",
       "      <td>epithelial</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>[[[212, 173, 228], [191, 151, 212], [172, 132,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9893</th>\n",
       "      <td>1627</td>\n",
       "      <td>60</td>\n",
       "      <td>1627.png</td>\n",
       "      <td>epithelial</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>[[[175, 141, 193], [191, 162, 205], [207, 180,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9894</th>\n",
       "      <td>1628</td>\n",
       "      <td>60</td>\n",
       "      <td>1628.png</td>\n",
       "      <td>epithelial</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>[[[227, 202, 232], [188, 159, 202], [180, 146,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9895</th>\n",
       "      <td>1629</td>\n",
       "      <td>60</td>\n",
       "      <td>1629.png</td>\n",
       "      <td>epithelial</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>[[[209, 180, 221], [242, 219, 247], [252, 237,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9896 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      InstanceID  patientID  ImageName cellTypeName  cellType  isCancerous   \n",
       "0          22405          1  22405.png   fibroblast         0            0  \\\n",
       "1          22406          1  22406.png   fibroblast         0            0   \n",
       "2          22407          1  22407.png   fibroblast         0            0   \n",
       "3          22408          1  22408.png   fibroblast         0            0   \n",
       "4          22409          1  22409.png   fibroblast         0            0   \n",
       "...          ...        ...        ...          ...       ...          ...   \n",
       "9891        1625         60   1625.png   epithelial         2            1   \n",
       "9892        1626         60   1626.png   epithelial         2            1   \n",
       "9893        1627         60   1627.png   epithelial         2            1   \n",
       "9894        1628         60   1628.png   epithelial         2            1   \n",
       "9895        1629         60   1629.png   epithelial         2            1   \n",
       "\n",
       "                                             image_data  \n",
       "0     [[[242, 213, 237], [239, 210, 234], [236, 206,...  \n",
       "1     [[[221, 175, 211], [223, 175, 211], [229, 180,...  \n",
       "2     [[[247, 243, 247], [248, 242, 246], [247, 240,...  \n",
       "3     [[[243, 242, 242], [243, 241, 241], [243, 241,...  \n",
       "4     [[[189, 130, 189], [187, 127, 185], [187, 126,...  \n",
       "...                                                 ...  \n",
       "9891  [[[171, 138, 180], [206, 175, 217], [238, 209,...  \n",
       "9892  [[[212, 173, 228], [191, 151, 212], [172, 132,...  \n",
       "9893  [[[175, 141, 193], [191, 162, 205], [207, 180,...  \n",
       "9894  [[[227, 202, 232], [188, 159, 202], [180, 146,...  \n",
       "9895  [[[209, 180, 221], [242, 219, 247], [252, 237,...  \n",
       "\n",
       "[9896 rows x 7 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the CSV file and create a dataframe\n",
    "main_csv_file = \"data_labels_mainData.csv\"\n",
    "extra_csv_file =\"data_labels_extraData.csv\"\n",
    "data = pd.read_csv(main_csv_file)\n",
    "extra_data = pd.read_csv(extra_csv_file)\n",
    "# Read the images from the directory and convert them to arrays\n",
    "image_dir = \"patch_images\"\n",
    "image_arrays = []\n",
    "for image_name in data[\"ImageName\"]:\n",
    "    img = cv2.imread(os.path.join(image_dir, image_name))\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img = cv2.resize(img, (32, 32))\n",
    "    image_arrays.append(img)\n",
    "image_arrays = np.array(image_arrays)\n",
    "\n",
    "#Merge the image arrays with the CSV dataframe\n",
    "data[\"image_data\"] = list(image_arrays)\n",
    "# data = data.drop(\"ImageName\", axis=1)\n",
    "data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to get the accuracy on the test data and get prediction on the extra dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_extra_data(model):\n",
    "    image_names = extra_data[\"ImageName\"]\n",
    "    for i in range(10):\n",
    "        image_loc = \"patch_images/\"+ str(image_names[i])\n",
    "        new_image = cv2.imread(image_loc)\n",
    "        new_image = cv2.cvtColor(new_image, cv2.COLOR_BGR2RGB)\n",
    "        new_image = cv2.resize(new_image, (32, 32))\n",
    "        new_image = np.array(new_image)\n",
    "        new_image = np.expand_dims(new_image, axis=0)\n",
    "        prediction = model.predict(new_image)[0][0]\n",
    "        if prediction >= 0.5:\n",
    "            print(\"The new image is predicted to be cancerous.\")\n",
    "            print(prediction)\n",
    "        else:\n",
    "            print(\"The new image is predicted to be non-cancerous.\")\n",
    "            print(prediction)\n",
    "\n",
    "def get_accuracy_and_loss(model,X_test,y_test,X_train,y_train):\n",
    "    val_loss, val_acc = model.evaluate(np.stack(X_test[\"image_data\"]), y_test)\n",
    "    print(\"Validation loss:\", val_loss)\n",
    "    print(\"Validation accuracy:\", val_acc)\n",
    "    # Evaluate on the training set\n",
    "    \n",
    "    train_loss, train_acc = model.evaluate(np.stack(X_train[\"image_data\"]), y_train)\n",
    "    print(\"Training loss:\", train_loss)\n",
    "    print(\"Training accuracy:\", train_acc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the data into train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ImageName</th>\n",
       "      <th>cellTypeName</th>\n",
       "      <th>cellType</th>\n",
       "      <th>image_data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22405.png</td>\n",
       "      <td>fibroblast</td>\n",
       "      <td>0</td>\n",
       "      <td>[[[242, 213, 237], [239, 210, 234], [236, 206,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>22406.png</td>\n",
       "      <td>fibroblast</td>\n",
       "      <td>0</td>\n",
       "      <td>[[[221, 175, 211], [223, 175, 211], [229, 180,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>22407.png</td>\n",
       "      <td>fibroblast</td>\n",
       "      <td>0</td>\n",
       "      <td>[[[247, 243, 247], [248, 242, 246], [247, 240,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>22408.png</td>\n",
       "      <td>fibroblast</td>\n",
       "      <td>0</td>\n",
       "      <td>[[[243, 242, 242], [243, 241, 241], [243, 241,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>22409.png</td>\n",
       "      <td>fibroblast</td>\n",
       "      <td>0</td>\n",
       "      <td>[[[189, 130, 189], [187, 127, 185], [187, 126,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9891</th>\n",
       "      <td>1625.png</td>\n",
       "      <td>epithelial</td>\n",
       "      <td>2</td>\n",
       "      <td>[[[171, 138, 180], [206, 175, 217], [238, 209,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9892</th>\n",
       "      <td>1626.png</td>\n",
       "      <td>epithelial</td>\n",
       "      <td>2</td>\n",
       "      <td>[[[212, 173, 228], [191, 151, 212], [172, 132,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9893</th>\n",
       "      <td>1627.png</td>\n",
       "      <td>epithelial</td>\n",
       "      <td>2</td>\n",
       "      <td>[[[175, 141, 193], [191, 162, 205], [207, 180,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9894</th>\n",
       "      <td>1628.png</td>\n",
       "      <td>epithelial</td>\n",
       "      <td>2</td>\n",
       "      <td>[[[227, 202, 232], [188, 159, 202], [180, 146,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9895</th>\n",
       "      <td>1629.png</td>\n",
       "      <td>epithelial</td>\n",
       "      <td>2</td>\n",
       "      <td>[[[209, 180, 221], [242, 219, 247], [252, 237,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9896 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      ImageName cellTypeName  cellType   \n",
       "0     22405.png   fibroblast         0  \\\n",
       "1     22406.png   fibroblast         0   \n",
       "2     22407.png   fibroblast         0   \n",
       "3     22408.png   fibroblast         0   \n",
       "4     22409.png   fibroblast         0   \n",
       "...         ...          ...       ...   \n",
       "9891   1625.png   epithelial         2   \n",
       "9892   1626.png   epithelial         2   \n",
       "9893   1627.png   epithelial         2   \n",
       "9894   1628.png   epithelial         2   \n",
       "9895   1629.png   epithelial         2   \n",
       "\n",
       "                                             image_data  \n",
       "0     [[[242, 213, 237], [239, 210, 234], [236, 206,...  \n",
       "1     [[[221, 175, 211], [223, 175, 211], [229, 180,...  \n",
       "2     [[[247, 243, 247], [248, 242, 246], [247, 240,...  \n",
       "3     [[[243, 242, 242], [243, 241, 241], [243, 241,...  \n",
       "4     [[[189, 130, 189], [187, 127, 185], [187, 126,...  \n",
       "...                                                 ...  \n",
       "9891  [[[171, 138, 180], [206, 175, 217], [238, 209,...  \n",
       "9892  [[[212, 173, 228], [191, 151, 212], [172, 132,...  \n",
       "9893  [[[175, 141, 193], [191, 162, 205], [207, 180,...  \n",
       "9894  [[[227, 202, 232], [188, 159, 202], [180, 146,...  \n",
       "9895  [[[209, 180, 221], [242, 219, 247], [252, 237,...  \n",
       "\n",
       "[9896 rows x 4 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X = data.drop(\"isCancerous\", axis=1)\n",
    "X = X.drop(\"InstanceID\",axis=1)\n",
    "X = X.drop(\"patientID\",axis=1)\n",
    "y = data[\"isCancerous\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "display(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 88.74% (+/- 1.85%)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "\n",
    "# define the number of folds for cross validation\n",
    "k = 5\n",
    "\n",
    "# create a KFold object with k folds\n",
    "kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "\n",
    "# create empty lists to store the accuracy scores for each fold\n",
    "scores = []\n",
    "\n",
    "# loop over each fold\n",
    "for train_index, test_index in kf.split(X):\n",
    "    # get the training and testing data for this fold\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    # preprocess the data\n",
    "    image_dir = \"patch_images\"\n",
    "    image_arrays_train = []\n",
    "    for image_name in X_train[\"ImageName\"]:\n",
    "        img = cv2.imread(os.path.join(image_dir, image_name))\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        img = cv2.resize(img, (32, 32))\n",
    "        image_arrays_train.append(img)\n",
    "    image_arrays_train = np.array(image_arrays_train)\n",
    "\n",
    "    image_arrays_test = []\n",
    "    for image_name in X_test[\"ImageName\"]:\n",
    "        img = cv2.imread(os.path.join(image_dir, image_name))\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        img = cv2.resize(img, (32, 32))\n",
    "        image_arrays_test.append(img)\n",
    "    image_arrays_test = np.array(image_arrays_test)\n",
    "\n",
    "    # define the model\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, kernel_size=(3, 3), activation=\"relu\", input_shape=(32, 32, 3)))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Conv2D(64, kernel_size=(3, 3), activation=\"relu\"))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation=\"relu\"))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1, activation=\"sigmoid\"))\n",
    "    model.compile(loss=\"binary_crossentropy\", optimizer=Adadelta(learning_rate=1e-1), metrics=[\"accuracy\"])\n",
    "\n",
    "    # train the model\n",
    "    model.fit(image_arrays_train, y_train, epochs=30, batch_size=32, verbose=0)\n",
    "\n",
    "    # evaluate the model on the test data for this fold\n",
    "    score = model.evaluate(image_arrays_test, y_test, verbose=0)\n",
    "    scores.append(score[1])\n",
    "\n",
    "# print the mean and standard deviation of the accuracy scores for all folds\n",
    "print(\"Accuracy: %.2f%% (+/- %.2f%%)\" % (np.mean(scores)*100, np.std(scores)*100))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BASELINE RGB model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "248/248 [==============================] - 5s 17ms/step - loss: 3.4774 - accuracy: 0.6807\n",
      "Epoch 2/30\n",
      "248/248 [==============================] - 4s 17ms/step - loss: 0.4905 - accuracy: 0.7718\n",
      "Epoch 3/30\n",
      "248/248 [==============================] - 4s 16ms/step - loss: 0.4330 - accuracy: 0.8190\n",
      "Epoch 4/30\n",
      "248/248 [==============================] - 4s 16ms/step - loss: 0.3950 - accuracy: 0.8354\n",
      "Epoch 5/30\n",
      "248/248 [==============================] - 4s 15ms/step - loss: 0.3682 - accuracy: 0.8484\n",
      "Epoch 6/30\n",
      "248/248 [==============================] - 4s 15ms/step - loss: 0.3562 - accuracy: 0.8545\n",
      "Epoch 7/30\n",
      "248/248 [==============================] - 4s 16ms/step - loss: 0.3412 - accuracy: 0.8588\n",
      "Epoch 8/30\n",
      "248/248 [==============================] - 4s 15ms/step - loss: 0.3296 - accuracy: 0.8661\n",
      "Epoch 9/30\n",
      "248/248 [==============================] - 4s 15ms/step - loss: 0.3365 - accuracy: 0.8669\n",
      "Epoch 10/30\n",
      "248/248 [==============================] - 4s 15ms/step - loss: 0.3314 - accuracy: 0.8719\n",
      "Epoch 11/30\n",
      "248/248 [==============================] - 4s 16ms/step - loss: 0.3172 - accuracy: 0.8738\n",
      "Epoch 12/30\n",
      "248/248 [==============================] - 4s 15ms/step - loss: 0.3111 - accuracy: 0.8746\n",
      "Epoch 13/30\n",
      "248/248 [==============================] - 4s 16ms/step - loss: 0.2976 - accuracy: 0.8823\n",
      "Epoch 14/30\n",
      "248/248 [==============================] - 4s 16ms/step - loss: 0.3080 - accuracy: 0.8755\n",
      "Epoch 15/30\n",
      "248/248 [==============================] - 4s 15ms/step - loss: 0.3012 - accuracy: 0.8791\n",
      "Epoch 16/30\n",
      "248/248 [==============================] - 4s 15ms/step - loss: 0.2940 - accuracy: 0.8824\n",
      "Epoch 17/30\n",
      "248/248 [==============================] - 4s 16ms/step - loss: 0.2865 - accuracy: 0.8839\n",
      "Epoch 18/30\n",
      "248/248 [==============================] - 4s 16ms/step - loss: 0.2937 - accuracy: 0.8840\n",
      "Epoch 19/30\n",
      "248/248 [==============================] - 4s 17ms/step - loss: 0.2877 - accuracy: 0.8866\n",
      "Epoch 20/30\n",
      "248/248 [==============================] - 5s 22ms/step - loss: 0.2866 - accuracy: 0.8870\n",
      "Epoch 21/30\n",
      "248/248 [==============================] - 5s 21ms/step - loss: 0.2819 - accuracy: 0.8880\n",
      "Epoch 22/30\n",
      "248/248 [==============================] - 4s 17ms/step - loss: 0.2753 - accuracy: 0.8891\n",
      "Epoch 23/30\n",
      "248/248 [==============================] - 4s 16ms/step - loss: 0.2791 - accuracy: 0.8895\n",
      "Epoch 24/30\n",
      "248/248 [==============================] - 4s 16ms/step - loss: 0.2809 - accuracy: 0.8897\n",
      "Epoch 25/30\n",
      "248/248 [==============================] - 4s 16ms/step - loss: 0.2677 - accuracy: 0.8920\n",
      "Epoch 26/30\n",
      "248/248 [==============================] - 4s 16ms/step - loss: 0.2639 - accuracy: 0.8953\n",
      "Epoch 27/30\n",
      "248/248 [==============================] - 4s 16ms/step - loss: 0.2678 - accuracy: 0.8945\n",
      "Epoch 28/30\n",
      "248/248 [==============================] - 4s 18ms/step - loss: 0.2713 - accuracy: 0.8942\n",
      "Epoch 29/30\n",
      "248/248 [==============================] - 4s 16ms/step - loss: 0.2611 - accuracy: 0.8953\n",
      "Epoch 30/30\n",
      "248/248 [==============================] - 4s 16ms/step - loss: 0.2574 - accuracy: 0.8991\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x28b41c4cac0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build and train a model using the merged dataset\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3), activation=\"relu\", input_shape=(32, 32, 3)))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Conv2D(64, kernel_size=(3, 3), activation=\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation=\"relu\"))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation=\"sigmoid\"))\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=Adadelta(learning_rate=1e-1), metrics=[\"accuracy\"])\n",
    "model.fit(np.stack(X_train[\"image_data\"]), y_train, epochs=30, batch_size=32)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62/62 [==============================] - 1s 8ms/step - loss: 0.2860 - accuracy: 0.8909\n",
      "Validation loss: 0.28602665662765503\n",
      "Validation accuracy: 0.8908539414405823\n",
      "248/248 [==============================] - 2s 7ms/step - loss: 0.2434 - accuracy: 0.9039\n",
      "Training loss: 0.24343055486679077\n",
      "Training accuracy: 0.9038777351379395\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "The new image is predicted to be non-cancerous.\n",
      "0.00023519235\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "The new image is predicted to be non-cancerous.\n",
      "0.015302624\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "The new image is predicted to be non-cancerous.\n",
      "0.027006261\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "The new image is predicted to be non-cancerous.\n",
      "0.006662449\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "The new image is predicted to be non-cancerous.\n",
      "0.010664831\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "The new image is predicted to be non-cancerous.\n",
      "0.0001616606\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "The new image is predicted to be non-cancerous.\n",
      "0.0008964677\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "The new image is predicted to be non-cancerous.\n",
      "0.012237194\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "The new image is predicted to be non-cancerous.\n",
      "0.014636761\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "The new image is predicted to be non-cancerous.\n",
      "0.0021006442\n"
     ]
    }
   ],
   "source": [
    "get_accuracy_and_loss(model,X_test,y_test,X_train,y_train)\n",
    "predict_extra_data(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 17ms/step\n",
      "The new image is predicted to be non-cancerous.\n",
      "[[0.00188357]]\n"
     ]
    }
   ],
   "source": [
    "new_image = cv2.imread(\"patch_images/22409.png\")\n",
    "new_image = cv2.cvtColor(new_image, cv2.COLOR_BGR2RGB)\n",
    "new_image = cv2.resize(new_image, (32, 32))\n",
    "new_image = np.array(new_image)\n",
    "new_image = np.expand_dims(new_image, axis=0)\n",
    "prediction = model.predict(new_image)\n",
    "if prediction >= 0.5:\n",
    "    print(\"The new image is predicted to be cancerous.\")\n",
    "    print(prediction)\n",
    "else:\n",
    "    print(\"The new image is predicted to be non-cancerous.\")\n",
    "    print(prediction)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RGB model with Data Augmentation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "247/247 [==============================] - 6s 20ms/step - loss: 1.1107 - accuracy: 0.7552\n",
      "Epoch 2/20\n",
      "247/247 [==============================] - 5s 20ms/step - loss: 0.4162 - accuracy: 0.8272\n",
      "Epoch 3/20\n",
      "247/247 [==============================] - 5s 21ms/step - loss: 0.3804 - accuracy: 0.8412\n",
      "Epoch 4/20\n",
      "247/247 [==============================] - 5s 20ms/step - loss: 0.3691 - accuracy: 0.8506\n",
      "Epoch 5/20\n",
      "247/247 [==============================] - 5s 20ms/step - loss: 0.3508 - accuracy: 0.8597\n",
      "Epoch 6/20\n",
      "247/247 [==============================] - 5s 20ms/step - loss: 0.3455 - accuracy: 0.8549\n",
      "Epoch 7/20\n",
      "247/247 [==============================] - 5s 20ms/step - loss: 0.3357 - accuracy: 0.8603\n",
      "Epoch 8/20\n",
      "247/247 [==============================] - 5s 20ms/step - loss: 0.3320 - accuracy: 0.8643\n",
      "Epoch 9/20\n",
      "247/247 [==============================] - 5s 20ms/step - loss: 0.3147 - accuracy: 0.8693\n",
      "Epoch 10/20\n",
      "247/247 [==============================] - 5s 20ms/step - loss: 0.3219 - accuracy: 0.8710\n",
      "Epoch 11/20\n",
      "247/247 [==============================] - 5s 20ms/step - loss: 0.3161 - accuracy: 0.8704\n",
      "Epoch 12/20\n",
      "247/247 [==============================] - 5s 20ms/step - loss: 0.3158 - accuracy: 0.8694\n",
      "Epoch 13/20\n",
      "247/247 [==============================] - 5s 21ms/step - loss: 0.3205 - accuracy: 0.8705\n",
      "Epoch 14/20\n",
      "247/247 [==============================] - 5s 20ms/step - loss: 0.3148 - accuracy: 0.8727\n",
      "Epoch 15/20\n",
      "247/247 [==============================] - 5s 20ms/step - loss: 0.3067 - accuracy: 0.8725\n",
      "Epoch 16/20\n",
      "247/247 [==============================] - 5s 19ms/step - loss: 0.3125 - accuracy: 0.8738\n",
      "Epoch 17/20\n",
      "247/247 [==============================] - 5s 21ms/step - loss: 0.3058 - accuracy: 0.8785\n",
      "Epoch 18/20\n",
      "247/247 [==============================] - 5s 20ms/step - loss: 0.3027 - accuracy: 0.8796\n",
      "Epoch 19/20\n",
      "247/247 [==============================] - 5s 21ms/step - loss: 0.3024 - accuracy: 0.8752\n",
      "Epoch 20/20\n",
      "247/247 [==============================] - 5s 20ms/step - loss: 0.2945 - accuracy: 0.8830\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1f2eeab0d90>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create an instance of the ImageDataGenerator class and specify the transformations to apply\n",
    "datagen = ImageDataGenerator( \n",
    "    zoom_range=0.2, \n",
    "    width_shift_range=0.01, # shift the image horizontally by up to 10%\n",
    "    height_shift_range=0.01, # shift the image vertically by up to 10%\n",
    "    shear_range=0.02, # shear the image by up to 10%\n",
    "    horizontal_flip=True, # flip the image horizontally\n",
    "    fill_mode='nearest') # fill any empty pixels with the nearest value\n",
    "\n",
    "# Fit the generator on the training data\n",
    "datagen.fit(np.stack(X_train[\"image_data\"]))\n",
    "\n",
    "data_aug_model = Sequential()\n",
    "data_aug_model.add(Conv2D(32, kernel_size=(3, 3), activation=\"relu\", input_shape=(32, 32, 3)))\n",
    "data_aug_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "data_aug_model.add(Conv2D(64, kernel_size=(3, 3), activation=\"relu\"))\n",
    "data_aug_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "data_aug_model.add(Flatten())\n",
    "data_aug_model.add(Dense(128, activation=\"relu\"))\n",
    "data_aug_model.add(Dropout(0.5))\n",
    "data_aug_model.add(Dense(1, activation=\"sigmoid\"))\n",
    "data_aug_model.compile(loss=\"binary_crossentropy\", optimizer=RMSprop(learning_rate=1e-4), metrics=[\"accuracy\"])\n",
    "\n",
    "# fit the model using the generator\n",
    "data_aug_model.fit(datagen.flow(np.stack(X_train[\"image_data\"]), y_train, batch_size=32),\n",
    "                    steps_per_epoch=len(X_train) / 32, epochs=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 54ms/step\n",
      "The new image is predicted to be non-cancerous.\n",
      "[[0.14547583]]\n"
     ]
    }
   ],
   "source": [
    "new_image = cv2.imread(\"patch_images/12681.png\")\n",
    "new_image = cv2.cvtColor(new_image, cv2.COLOR_BGR2RGB)\n",
    "new_image = cv2.resize(new_image, (32, 32))\n",
    "new_image = np.array(new_image)\n",
    "new_image = np.expand_dims(new_image, axis=0)\n",
    "prediction = data_aug_model.predict(new_image)\n",
    "if prediction >= 0.5:\n",
    "    print(\"The new image is predicted to be cancerous.\")\n",
    "    print(prediction)\n",
    "else:\n",
    "    print(\"The new image is predicted to be non-cancerous.\")\n",
    "    print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 17ms/step\n",
      "The new image is predicted to be non-cancerous.\n",
      "[[0.14547583]]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "The new image is predicted to be non-cancerous.\n",
      "[[0.29256076]]\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "The new image is predicted to be non-cancerous.\n",
      "[[0.15583122]]\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "The new image is predicted to be non-cancerous.\n",
      "[[0.1505906]]\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "The new image is predicted to be non-cancerous.\n",
      "[[0.35594097]]\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "The new image is predicted to be non-cancerous.\n",
      "[[0.11479353]]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "The new image is predicted to be non-cancerous.\n",
      "[[0.17687058]]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "The new image is predicted to be non-cancerous.\n",
      "[[0.41733012]]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "The new image is predicted to be non-cancerous.\n",
      "[[0.26824713]]\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "The new image is predicted to be non-cancerous.\n",
      "[[0.06410068]]\n",
      "62/62 [==============================] - 1s 8ms/step - loss: 0.2987 - accuracy: 0.8753\n",
      "Validation loss: 0.29866981506347656\n",
      "Validation accuracy: 0.8752525448799133\n",
      "248/248 [==============================] - 2s 6ms/step - loss: 0.2941 - accuracy: 0.8663\n",
      "Training loss: 0.294100821018219\n",
      "Training accuracy: 0.8663466572761536\n"
     ]
    }
   ],
   "source": [
    "predict_extra_data(data_aug_model)\n",
    "get_accuracy_and_loss(data_aug_model,X_test,y_test,X_train,y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RGB model with a deeper network (more layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "248/248 [==============================] - 5s 19ms/step - loss: 1.4587 - accuracy: 0.6565\n",
      "Epoch 2/30\n",
      "248/248 [==============================] - 5s 19ms/step - loss: 0.4918 - accuracy: 0.7871\n",
      "Epoch 3/30\n",
      "248/248 [==============================] - 4s 18ms/step - loss: 0.4225 - accuracy: 0.8201\n",
      "Epoch 4/30\n",
      "248/248 [==============================] - 5s 20ms/step - loss: 0.3903 - accuracy: 0.8355\n",
      "Epoch 5/30\n",
      "248/248 [==============================] - 5s 20ms/step - loss: 0.3725 - accuracy: 0.8447\n",
      "Epoch 6/30\n",
      "248/248 [==============================] - 5s 20ms/step - loss: 0.3506 - accuracy: 0.8514\n",
      "Epoch 7/30\n",
      "248/248 [==============================] - 5s 19ms/step - loss: 0.3359 - accuracy: 0.8638\n",
      "Epoch 8/30\n",
      "248/248 [==============================] - 5s 19ms/step - loss: 0.3340 - accuracy: 0.8660\n",
      "Epoch 9/30\n",
      "248/248 [==============================] - 5s 20ms/step - loss: 0.3259 - accuracy: 0.8681\n",
      "Epoch 10/30\n",
      "248/248 [==============================] - 5s 20ms/step - loss: 0.3168 - accuracy: 0.8742\n",
      "Epoch 11/30\n",
      "248/248 [==============================] - 5s 21ms/step - loss: 0.3144 - accuracy: 0.8747\n",
      "Epoch 12/30\n",
      "248/248 [==============================] - 5s 20ms/step - loss: 0.3104 - accuracy: 0.8733\n",
      "Epoch 13/30\n",
      "248/248 [==============================] - 5s 19ms/step - loss: 0.3066 - accuracy: 0.8761\n",
      "Epoch 14/30\n",
      "248/248 [==============================] - 5s 19ms/step - loss: 0.3043 - accuracy: 0.8757\n",
      "Epoch 15/30\n",
      "248/248 [==============================] - 4s 18ms/step - loss: 0.2936 - accuracy: 0.8809\n",
      "Epoch 16/30\n",
      "248/248 [==============================] - 5s 19ms/step - loss: 0.2970 - accuracy: 0.8815\n",
      "Epoch 17/30\n",
      "248/248 [==============================] - 5s 18ms/step - loss: 0.2866 - accuracy: 0.8852\n",
      "Epoch 18/30\n",
      "248/248 [==============================] - 5s 19ms/step - loss: 0.2909 - accuracy: 0.8876\n",
      "Epoch 19/30\n",
      "248/248 [==============================] - 5s 18ms/step - loss: 0.2842 - accuracy: 0.8854\n",
      "Epoch 20/30\n",
      "248/248 [==============================] - 5s 18ms/step - loss: 0.2883 - accuracy: 0.8886\n",
      "Epoch 21/30\n",
      "248/248 [==============================] - 5s 19ms/step - loss: 0.2829 - accuracy: 0.8879\n",
      "Epoch 22/30\n",
      "248/248 [==============================] - 5s 18ms/step - loss: 0.2850 - accuracy: 0.8867\n",
      "Epoch 23/30\n",
      "248/248 [==============================] - 5s 20ms/step - loss: 0.2818 - accuracy: 0.8869\n",
      "Epoch 24/30\n",
      "248/248 [==============================] - 5s 21ms/step - loss: 0.2833 - accuracy: 0.8858\n",
      "Epoch 25/30\n",
      "248/248 [==============================] - 5s 21ms/step - loss: 0.2852 - accuracy: 0.8854\n",
      "Epoch 26/30\n",
      "248/248 [==============================] - 5s 21ms/step - loss: 0.2778 - accuracy: 0.8903\n",
      "Epoch 27/30\n",
      "248/248 [==============================] - 5s 21ms/step - loss: 0.2847 - accuracy: 0.8900\n",
      "Epoch 28/30\n",
      "248/248 [==============================] - 5s 21ms/step - loss: 0.2836 - accuracy: 0.8881\n",
      "Epoch 29/30\n",
      "248/248 [==============================] - 5s 21ms/step - loss: 0.2792 - accuracy: 0.8910\n",
      "Epoch 30/30\n",
      "248/248 [==============================] - 5s 21ms/step - loss: 0.2795 - accuracy: 0.8935\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1f280185330>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deeper_model = Sequential()\n",
    "deeper_model.add(Conv2D(32, kernel_size=(3, 3), activation=\"relu\", input_shape=(32, 32, 3)))\n",
    "deeper_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "deeper_model.add(Conv2D(64, kernel_size=(3, 3), activation=\"relu\"))\n",
    "deeper_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "deeper_model.add(Conv2D(128, kernel_size=(3, 3), activation=\"relu\"))\n",
    "deeper_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "deeper_model.add(Flatten())\n",
    "deeper_model.add(Dense(256, activation=\"relu\"))\n",
    "deeper_model.add(Dropout(0.5))\n",
    "deeper_model.add(Dense(128, activation=\"relu\"))\n",
    "deeper_model.add(Dropout(0.5))\n",
    "deeper_model.add(Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "deeper_model.compile(loss=\"binary_crossentropy\", optimizer=RMSprop(learning_rate=1e-4), metrics=[\"accuracy\"])\n",
    "\n",
    "deeper_model.fit(np.stack(X_train[\"image_data\"]), y_train, epochs=30, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 62ms/step\n",
      "The new image is predicted to be non-cancerous.\n",
      "[[0.01378987]]\n"
     ]
    }
   ],
   "source": [
    "new_image = cv2.imread(\"patch_images/22409.png\")\n",
    "new_image = cv2.cvtColor(new_image, cv2.COLOR_BGR2RGB)\n",
    "new_image = cv2.resize(new_image, (32, 32))\n",
    "new_image = np.array(new_image)\n",
    "new_image = np.expand_dims(new_image, axis=0)\n",
    "prediction = deeper_model.predict(new_image)\n",
    "if prediction >= 0.5:\n",
    "    print(\"The new image is predicted to be cancerous.\")\n",
    "    print(prediction)\n",
    "else:\n",
    "    print(\"The new image is predicted to be non-cancerous.\")\n",
    "    print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 16ms/step\n",
      "The new image is predicted to be non-cancerous.\n",
      "[[4.120697e-05]]\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "The new image is predicted to be non-cancerous.\n",
      "[[0.24716307]]\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "The new image is predicted to be non-cancerous.\n",
      "[[0.18089657]]\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "The new image is predicted to be non-cancerous.\n",
      "[[0.02440446]]\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "The new image is predicted to be non-cancerous.\n",
      "[[0.037422]]\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "The new image is predicted to be non-cancerous.\n",
      "[[0.03327738]]\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "The new image is predicted to be non-cancerous.\n",
      "[[0.03030089]]\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "The new image is predicted to be non-cancerous.\n",
      "[[0.07249901]]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "The new image is predicted to be non-cancerous.\n",
      "[[0.02834261]]\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "The new image is predicted to be non-cancerous.\n",
      "[[0.13245714]]\n",
      "62/62 [==============================] - 1s 7ms/step - loss: 0.3242 - accuracy: 0.9081\n",
      "Validation loss: 0.32419225573539734\n",
      "Validation accuracy: 0.9080808162689209\n",
      "248/248 [==============================] - 2s 7ms/step - loss: 0.2338 - accuracy: 0.9155\n",
      "Training loss: 0.2337709218263626\n",
      "Training accuracy: 0.9154876470565796\n"
     ]
    }
   ],
   "source": [
    "predict_extra_data(deeper_model)\n",
    "get_accuracy_and_loss(deeper_model,X_test,y_test,X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "248/248 [==============================] - 43s 166ms/step - loss: 0.5835 - accuracy: 0.7494 - val_loss: 2.6387 - val_accuracy: 0.3995\n",
      "Epoch 2/100\n",
      "248/248 [==============================] - 38s 152ms/step - loss: 0.4410 - accuracy: 0.8321 - val_loss: 2.2389 - val_accuracy: 0.4045\n",
      "Epoch 3/100\n",
      "248/248 [==============================] - 37s 151ms/step - loss: 0.4068 - accuracy: 0.8430 - val_loss: 1.7325 - val_accuracy: 0.5359\n",
      "Epoch 4/100\n",
      "248/248 [==============================] - 37s 151ms/step - loss: 0.3754 - accuracy: 0.8535 - val_loss: 2.4103 - val_accuracy: 0.4480\n",
      "Epoch 5/100\n",
      "248/248 [==============================] - 37s 151ms/step - loss: 0.3603 - accuracy: 0.8586 - val_loss: 0.9431 - val_accuracy: 0.6768\n",
      "Epoch 6/100\n",
      "248/248 [==============================] - 37s 151ms/step - loss: 0.3534 - accuracy: 0.8576 - val_loss: 0.7896 - val_accuracy: 0.7379\n",
      "Epoch 7/100\n",
      "248/248 [==============================] - 38s 151ms/step - loss: 0.3257 - accuracy: 0.8661 - val_loss: 0.3710 - val_accuracy: 0.8732\n",
      "Epoch 8/100\n",
      "248/248 [==============================] - 37s 150ms/step - loss: 0.3195 - accuracy: 0.8709 - val_loss: 0.5670 - val_accuracy: 0.7980\n",
      "Epoch 9/100\n",
      "248/248 [==============================] - 38s 153ms/step - loss: 0.3159 - accuracy: 0.8729 - val_loss: 1.0895 - val_accuracy: 0.7000\n",
      "Epoch 10/100\n",
      "248/248 [==============================] - 41s 164ms/step - loss: 0.3182 - accuracy: 0.8708 - val_loss: 0.5073 - val_accuracy: 0.8308\n",
      "Epoch 11/100\n",
      "248/248 [==============================] - 37s 150ms/step - loss: 0.3073 - accuracy: 0.8753 - val_loss: 0.6578 - val_accuracy: 0.7712\n",
      "Epoch 12/100\n",
      "248/248 [==============================] - 38s 152ms/step - loss: 0.2914 - accuracy: 0.8823 - val_loss: 0.3875 - val_accuracy: 0.8682\n",
      "Epoch 13/100\n",
      "248/248 [==============================] - 38s 153ms/step - loss: 0.2792 - accuracy: 0.8874 - val_loss: 0.2827 - val_accuracy: 0.9030\n",
      "Epoch 14/100\n",
      "248/248 [==============================] - 41s 164ms/step - loss: 0.2802 - accuracy: 0.8891 - val_loss: 0.4600 - val_accuracy: 0.8424\n",
      "Epoch 15/100\n",
      "248/248 [==============================] - 42s 168ms/step - loss: 0.2769 - accuracy: 0.8859 - val_loss: 0.3181 - val_accuracy: 0.8894\n",
      "Epoch 16/100\n",
      "248/248 [==============================] - 38s 152ms/step - loss: 0.2723 - accuracy: 0.8914 - val_loss: 0.3771 - val_accuracy: 0.8768\n",
      "Epoch 17/100\n",
      "248/248 [==============================] - 45s 181ms/step - loss: 0.2675 - accuracy: 0.8929 - val_loss: 0.4816 - val_accuracy: 0.8313\n",
      "Epoch 18/100\n",
      "248/248 [==============================] - 41s 165ms/step - loss: 0.2663 - accuracy: 0.8892 - val_loss: 0.2328 - val_accuracy: 0.9136\n",
      "Epoch 19/100\n",
      "248/248 [==============================] - 40s 161ms/step - loss: 0.2620 - accuracy: 0.8929 - val_loss: 0.2378 - val_accuracy: 0.9096\n",
      "Epoch 20/100\n",
      "248/248 [==============================] - 39s 159ms/step - loss: 0.2651 - accuracy: 0.8929 - val_loss: 0.6231 - val_accuracy: 0.8237\n",
      "Epoch 21/100\n",
      "248/248 [==============================] - 38s 155ms/step - loss: 0.2621 - accuracy: 0.8981 - val_loss: 0.2391 - val_accuracy: 0.9126\n",
      "Epoch 22/100\n",
      "248/248 [==============================] - 40s 159ms/step - loss: 0.2487 - accuracy: 0.8970 - val_loss: 0.2873 - val_accuracy: 0.8949\n",
      "Epoch 23/100\n",
      "248/248 [==============================] - 40s 160ms/step - loss: 0.2497 - accuracy: 0.8979 - val_loss: 0.2878 - val_accuracy: 0.8985\n",
      "Epoch 24/100\n",
      "248/248 [==============================] - 39s 157ms/step - loss: 0.2429 - accuracy: 0.9029 - val_loss: 0.3919 - val_accuracy: 0.8596\n",
      "Epoch 25/100\n",
      "248/248 [==============================] - 39s 156ms/step - loss: 0.2387 - accuracy: 0.9013 - val_loss: 0.2870 - val_accuracy: 0.8970\n",
      "Epoch 26/100\n",
      "248/248 [==============================] - 39s 157ms/step - loss: 0.2441 - accuracy: 0.9035 - val_loss: 0.2102 - val_accuracy: 0.9207\n",
      "Epoch 27/100\n",
      "248/248 [==============================] - 40s 160ms/step - loss: 0.2370 - accuracy: 0.9066 - val_loss: 0.3482 - val_accuracy: 0.8606\n",
      "Epoch 28/100\n",
      "248/248 [==============================] - 39s 159ms/step - loss: 0.2381 - accuracy: 0.9047 - val_loss: 0.2290 - val_accuracy: 0.9207\n",
      "Epoch 29/100\n",
      "248/248 [==============================] - 40s 159ms/step - loss: 0.2364 - accuracy: 0.9083 - val_loss: 0.2156 - val_accuracy: 0.9192\n",
      "Epoch 30/100\n",
      "248/248 [==============================] - 39s 157ms/step - loss: 0.2322 - accuracy: 0.9072 - val_loss: 0.2217 - val_accuracy: 0.9212\n",
      "Epoch 31/100\n",
      "248/248 [==============================] - 40s 160ms/step - loss: 0.2306 - accuracy: 0.9099 - val_loss: 0.5052 - val_accuracy: 0.8217\n",
      "Epoch 32/100\n",
      "248/248 [==============================] - 39s 158ms/step - loss: 0.2337 - accuracy: 0.9074 - val_loss: 0.3143 - val_accuracy: 0.8859\n",
      "Epoch 33/100\n",
      "248/248 [==============================] - 39s 159ms/step - loss: 0.2317 - accuracy: 0.9075 - val_loss: 0.2110 - val_accuracy: 0.9348\n",
      "Epoch 34/100\n",
      "248/248 [==============================] - 39s 157ms/step - loss: 0.2262 - accuracy: 0.9123 - val_loss: 0.3620 - val_accuracy: 0.8707\n",
      "Epoch 35/100\n",
      "248/248 [==============================] - 39s 156ms/step - loss: 0.2254 - accuracy: 0.9093 - val_loss: 0.2704 - val_accuracy: 0.8965\n",
      "Epoch 36/100\n",
      "248/248 [==============================] - 40s 160ms/step - loss: 0.2190 - accuracy: 0.9142 - val_loss: 0.2208 - val_accuracy: 0.9152\n",
      "Epoch 37/100\n",
      "248/248 [==============================] - 39s 159ms/step - loss: 0.2330 - accuracy: 0.9087 - val_loss: 0.2192 - val_accuracy: 0.9141\n",
      "Epoch 38/100\n",
      "248/248 [==============================] - 39s 157ms/step - loss: 0.2246 - accuracy: 0.9128 - val_loss: 0.2695 - val_accuracy: 0.9010\n",
      "Epoch 39/100\n",
      "248/248 [==============================] - 39s 158ms/step - loss: 0.2201 - accuracy: 0.9123 - val_loss: 0.2492 - val_accuracy: 0.9131\n",
      "Epoch 40/100\n",
      "248/248 [==============================] - 39s 157ms/step - loss: 0.2226 - accuracy: 0.9126 - val_loss: 0.2256 - val_accuracy: 0.9227\n",
      "Epoch 41/100\n",
      "248/248 [==============================] - 39s 157ms/step - loss: 0.2201 - accuracy: 0.9090 - val_loss: 0.2665 - val_accuracy: 0.9010\n",
      "Epoch 42/100\n",
      "248/248 [==============================] - 39s 157ms/step - loss: 0.2202 - accuracy: 0.9128 - val_loss: 0.2800 - val_accuracy: 0.9005\n",
      "Epoch 43/100\n",
      "248/248 [==============================] - 39s 157ms/step - loss: 0.2128 - accuracy: 0.9197 - val_loss: 0.2182 - val_accuracy: 0.9172\n",
      "Epoch 44/100\n",
      "248/248 [==============================] - 39s 157ms/step - loss: 0.2169 - accuracy: 0.9157 - val_loss: 0.2089 - val_accuracy: 0.9232\n",
      "Epoch 45/100\n",
      "248/248 [==============================] - 39s 156ms/step - loss: 0.2134 - accuracy: 0.9195 - val_loss: 0.4477 - val_accuracy: 0.8364\n",
      "Epoch 46/100\n",
      "248/248 [==============================] - 39s 157ms/step - loss: 0.2104 - accuracy: 0.9168 - val_loss: 0.2846 - val_accuracy: 0.8874\n",
      "Epoch 47/100\n",
      "248/248 [==============================] - 39s 157ms/step - loss: 0.2165 - accuracy: 0.9108 - val_loss: 0.2377 - val_accuracy: 0.9116\n",
      "Epoch 48/100\n",
      "248/248 [==============================] - 39s 157ms/step - loss: 0.2181 - accuracy: 0.9132 - val_loss: 0.1973 - val_accuracy: 0.9212\n",
      "Epoch 49/100\n",
      "248/248 [==============================] - 39s 157ms/step - loss: 0.2132 - accuracy: 0.9144 - val_loss: 0.2599 - val_accuracy: 0.9035\n",
      "Epoch 50/100\n",
      "248/248 [==============================] - 40s 160ms/step - loss: 0.2147 - accuracy: 0.9142 - val_loss: 0.2084 - val_accuracy: 0.9288\n",
      "Epoch 51/100\n",
      "248/248 [==============================] - 41s 164ms/step - loss: 0.2063 - accuracy: 0.9193 - val_loss: 0.1961 - val_accuracy: 0.9273\n",
      "Epoch 52/100\n",
      "248/248 [==============================] - 40s 160ms/step - loss: 0.2140 - accuracy: 0.9151 - val_loss: 0.2763 - val_accuracy: 0.9000\n",
      "Epoch 53/100\n",
      "248/248 [==============================] - 43s 172ms/step - loss: 0.2033 - accuracy: 0.9176 - val_loss: 0.2203 - val_accuracy: 0.9202\n",
      "Epoch 54/100\n",
      "248/248 [==============================] - 40s 160ms/step - loss: 0.2068 - accuracy: 0.9175 - val_loss: 0.2717 - val_accuracy: 0.9106\n",
      "Epoch 55/100\n",
      "248/248 [==============================] - 38s 153ms/step - loss: 0.2174 - accuracy: 0.9141 - val_loss: 0.4163 - val_accuracy: 0.8677\n",
      "Epoch 56/100\n",
      "248/248 [==============================] - 38s 153ms/step - loss: 0.2039 - accuracy: 0.9214 - val_loss: 0.3448 - val_accuracy: 0.8722\n",
      "Epoch 57/100\n",
      "248/248 [==============================] - 38s 154ms/step - loss: 0.2105 - accuracy: 0.9168 - val_loss: 0.1826 - val_accuracy: 0.9318\n",
      "Epoch 58/100\n",
      "248/248 [==============================] - 38s 154ms/step - loss: 0.2009 - accuracy: 0.9205 - val_loss: 0.3494 - val_accuracy: 0.8773\n",
      "Epoch 59/100\n",
      "248/248 [==============================] - 38s 154ms/step - loss: 0.2024 - accuracy: 0.9190 - val_loss: 0.2661 - val_accuracy: 0.8975\n",
      "Epoch 60/100\n",
      "248/248 [==============================] - 38s 153ms/step - loss: 0.2012 - accuracy: 0.9188 - val_loss: 0.2005 - val_accuracy: 0.9202\n",
      "Epoch 61/100\n",
      "248/248 [==============================] - 37s 151ms/step - loss: 0.2132 - accuracy: 0.9186 - val_loss: 0.1757 - val_accuracy: 0.9348\n",
      "Epoch 62/100\n",
      "248/248 [==============================] - 38s 152ms/step - loss: 0.2111 - accuracy: 0.9151 - val_loss: 0.1849 - val_accuracy: 0.9253\n",
      "Epoch 63/100\n",
      "248/248 [==============================] - 38s 152ms/step - loss: 0.2007 - accuracy: 0.9227 - val_loss: 0.2119 - val_accuracy: 0.9217\n",
      "Epoch 64/100\n",
      "248/248 [==============================] - 38s 152ms/step - loss: 0.1976 - accuracy: 0.9250 - val_loss: 0.2147 - val_accuracy: 0.9227\n",
      "Epoch 65/100\n",
      "248/248 [==============================] - 38s 154ms/step - loss: 0.2026 - accuracy: 0.9210 - val_loss: 0.2009 - val_accuracy: 0.9237\n",
      "Epoch 66/100\n",
      "248/248 [==============================] - 38s 155ms/step - loss: 0.1967 - accuracy: 0.9240 - val_loss: 0.2112 - val_accuracy: 0.9202\n",
      "Epoch 67/100\n",
      "248/248 [==============================] - 40s 161ms/step - loss: 0.2049 - accuracy: 0.9204 - val_loss: 0.2101 - val_accuracy: 0.9217\n",
      "Epoch 68/100\n",
      "248/248 [==============================] - 44s 177ms/step - loss: 0.2012 - accuracy: 0.9210 - val_loss: 0.3199 - val_accuracy: 0.8924\n",
      "Epoch 69/100\n",
      "248/248 [==============================] - 39s 158ms/step - loss: 0.2037 - accuracy: 0.9222 - val_loss: 0.2516 - val_accuracy: 0.9056\n",
      "Epoch 70/100\n",
      "248/248 [==============================] - 42s 168ms/step - loss: 0.1954 - accuracy: 0.9255 - val_loss: 0.2387 - val_accuracy: 0.9106\n",
      "Epoch 71/100\n",
      "248/248 [==============================] - 38s 152ms/step - loss: 0.1899 - accuracy: 0.9255 - val_loss: 0.2261 - val_accuracy: 0.9202\n",
      "Epoch 72/100\n",
      "248/248 [==============================] - 40s 160ms/step - loss: 0.1941 - accuracy: 0.9245 - val_loss: 0.2312 - val_accuracy: 0.9157\n",
      "Epoch 73/100\n",
      "248/248 [==============================] - 42s 168ms/step - loss: 0.1994 - accuracy: 0.9219 - val_loss: 0.2280 - val_accuracy: 0.9167\n",
      "Epoch 74/100\n",
      "248/248 [==============================] - 43s 175ms/step - loss: 0.1900 - accuracy: 0.9274 - val_loss: 0.2454 - val_accuracy: 0.9040\n",
      "Epoch 75/100\n",
      "248/248 [==============================] - 40s 162ms/step - loss: 0.1909 - accuracy: 0.9261 - val_loss: 0.2128 - val_accuracy: 0.9116\n",
      "Epoch 76/100\n",
      "248/248 [==============================] - 40s 163ms/step - loss: 0.1869 - accuracy: 0.9266 - val_loss: 0.1675 - val_accuracy: 0.9343\n",
      "Epoch 77/100\n",
      "248/248 [==============================] - 38s 151ms/step - loss: 0.1934 - accuracy: 0.9248 - val_loss: 0.2682 - val_accuracy: 0.8965\n",
      "Epoch 78/100\n",
      "248/248 [==============================] - 38s 154ms/step - loss: 0.1862 - accuracy: 0.9270 - val_loss: 0.1822 - val_accuracy: 0.9303\n",
      "Epoch 79/100\n",
      "248/248 [==============================] - 38s 154ms/step - loss: 0.1957 - accuracy: 0.9241 - val_loss: 0.2798 - val_accuracy: 0.8990\n",
      "Epoch 80/100\n",
      "248/248 [==============================] - 37s 150ms/step - loss: 0.1927 - accuracy: 0.9271 - val_loss: 0.2350 - val_accuracy: 0.9192\n",
      "Epoch 81/100\n",
      "248/248 [==============================] - 38s 155ms/step - loss: 0.2012 - accuracy: 0.9218 - val_loss: 0.1922 - val_accuracy: 0.9258\n",
      "Epoch 82/100\n",
      "248/248 [==============================] - 39s 157ms/step - loss: 0.1856 - accuracy: 0.9290 - val_loss: 0.2134 - val_accuracy: 0.9202\n",
      "Epoch 83/100\n",
      "248/248 [==============================] - 41s 167ms/step - loss: 0.1930 - accuracy: 0.9274 - val_loss: 0.2138 - val_accuracy: 0.9182\n",
      "Epoch 84/100\n",
      "248/248 [==============================] - 36s 147ms/step - loss: 0.1876 - accuracy: 0.9270 - val_loss: 0.2214 - val_accuracy: 0.9131\n",
      "Epoch 85/100\n",
      "248/248 [==============================] - 36s 147ms/step - loss: 0.1932 - accuracy: 0.9253 - val_loss: 0.3436 - val_accuracy: 0.8737\n",
      "Epoch 86/100\n",
      "248/248 [==============================] - 40s 162ms/step - loss: 0.1910 - accuracy: 0.9277 - val_loss: 0.2071 - val_accuracy: 0.9232\n",
      "Epoch 87/100\n",
      "248/248 [==============================] - 36s 146ms/step - loss: 0.1854 - accuracy: 0.9306 - val_loss: 0.1841 - val_accuracy: 0.9278\n",
      "Epoch 88/100\n",
      "248/248 [==============================] - 36s 146ms/step - loss: 0.1857 - accuracy: 0.9289 - val_loss: 0.2644 - val_accuracy: 0.9081\n",
      "Epoch 89/100\n",
      "248/248 [==============================] - 36s 147ms/step - loss: 0.1944 - accuracy: 0.9262 - val_loss: 0.1981 - val_accuracy: 0.9263\n",
      "Epoch 90/100\n",
      "248/248 [==============================] - 36s 145ms/step - loss: 0.1852 - accuracy: 0.9275 - val_loss: 0.3537 - val_accuracy: 0.8621\n",
      "Epoch 91/100\n",
      "248/248 [==============================] - 36s 146ms/step - loss: 0.1910 - accuracy: 0.9295 - val_loss: 0.1986 - val_accuracy: 0.9273\n",
      "Epoch 92/100\n",
      "248/248 [==============================] - 41s 163ms/step - loss: 0.1880 - accuracy: 0.9301 - val_loss: 0.3231 - val_accuracy: 0.8823\n",
      "Epoch 93/100\n",
      "248/248 [==============================] - 38s 154ms/step - loss: 0.1789 - accuracy: 0.9294 - val_loss: 0.2630 - val_accuracy: 0.9040\n",
      "Epoch 94/100\n",
      "248/248 [==============================] - 40s 160ms/step - loss: 0.1789 - accuracy: 0.9336 - val_loss: 0.2740 - val_accuracy: 0.9116\n",
      "Epoch 95/100\n",
      "248/248 [==============================] - 41s 164ms/step - loss: 0.1886 - accuracy: 0.9291 - val_loss: 0.2188 - val_accuracy: 0.9071\n",
      "Epoch 96/100\n",
      "248/248 [==============================] - 44s 177ms/step - loss: 0.1776 - accuracy: 0.9317 - val_loss: 0.2172 - val_accuracy: 0.9207\n",
      "Epoch 97/100\n",
      "248/248 [==============================] - 51s 206ms/step - loss: 0.1905 - accuracy: 0.9275 - val_loss: 0.2040 - val_accuracy: 0.9258\n",
      "Epoch 98/100\n",
      "248/248 [==============================] - 51s 205ms/step - loss: 0.1881 - accuracy: 0.9276 - val_loss: 0.2018 - val_accuracy: 0.9273\n",
      "Epoch 99/100\n",
      "248/248 [==============================] - 46s 184ms/step - loss: 0.1892 - accuracy: 0.9274 - val_loss: 0.1707 - val_accuracy: 0.9323\n",
      "Epoch 100/100\n",
      "248/248 [==============================] - 39s 157ms/step - loss: 0.1812 - accuracy: 0.9318 - val_loss: 0.2491 - val_accuracy: 0.8995\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1f2cfd4d6f0>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_file = \"data_labels_mainData.csv\"\n",
    "data = pd.read_csv(csv_file)\n",
    "\n",
    "# Step 2: Read the images from the directory and convert them to arrays\n",
    "image_dir = \"patch_images\"\n",
    "image_arrays = []\n",
    "for image_name in data[\"ImageName\"]:\n",
    "    image_path = os.path.join(image_dir, image_name)\n",
    "    img = cv2.imread(image_path, cv2.COLOR_BGR2RGB)\n",
    "    img = cv2.resize(img, (32, 32)) / 255.0\n",
    "    image_arrays.append(img)\n",
    "image_arrays = np.array(image_arrays)\n",
    "\n",
    "# Step 3: Merge the image arrays with the CSV dataframe\n",
    "data[\"image_data\"] = list(image_arrays)\n",
    "data = data.drop(\"ImageName\", axis=1)\n",
    "\n",
    "# Step 4: Split the data into training and testing sets\n",
    "X = data.drop(\"isCancerous\", axis=1)\n",
    "y = data[\"isCancerous\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 5: Create an ImageDataGenerator instance for data augmentation\n",
    "train_datagen = ImageDataGenerator(rotation_range=20, width_shift_range=0.2,\n",
    "                                   height_shift_range=0.2, shear_range=0.2, zoom_range=0.2,\n",
    "                                   horizontal_flip=True, vertical_flip=True)\n",
    "\n",
    "test_datagen = ImageDataGenerator()\n",
    "\n",
    "# Step 6: Create a generator for training data using the ImageDataGenerator instance\n",
    "train_generator = train_datagen.flow(np.stack(X_train[\"image_data\"]), y_train, batch_size=32)\n",
    "\n",
    "# Step 7: Create a generator for validation data using the ImageDataGenerator instance\n",
    "valid_generator = test_datagen.flow(np.stack(X_test[\"image_data\"]), y_test, batch_size=32)\n",
    "\n",
    "# Step 8: Build and train a model using the merged dataset and data augmentation\n",
    "model = Sequential()\n",
    "\n",
    "# Add convolutional layers\n",
    "model.add(Conv2D(32, kernel_size=(3, 3), activation=\"relu\", input_shape=(32, 32, 3), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(32, kernel_size=(3, 3), activation=\"relu\", padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(64, kernel_size=(3, 3), activation=\"relu\", padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(64, kernel_size=(3, 3), activation=\"relu\", padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(128, kernel_size=(3, 3), activation=\"relu\", padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(128, kernel_size=(3, 3), activation=\"relu\", padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(256, kernel_size=(3, 3), activation=\"relu\", padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(256, kernel_size=(3, 3), activation=\"relu\", padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "# Flatten and add dense layers\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512, activation=\"relu\"))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(256, activation=\"relu\"))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=RMSprop(learning_rate=1e-4), metrics=[\"accuracy\"])\n",
    "\n",
    "# Fit the model using data augmentation\n",
    "model.fit(train_generator, epochs=100, validation_data=valid_generator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 22ms/step\n",
      "The new image is predicted to be non-cancerous.\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "new_image = cv2.imread(\"patch_images/22409.png\")\n",
    "new_image = cv2.cvtColor(new_image, cv2.COLOR_BGR2RGB)\n",
    "new_image = cv2.resize(new_image, (32, 32))\n",
    "new_image = np.array(new_image)\n",
    "new_image = np.expand_dims(new_image, axis=0)\n",
    "prediction = model.predict(new_image)[0][0]\n",
    "if prediction >= 0.5:\n",
    "    print(\"The new image is predicted to be cancerous.\")\n",
    "    print(prediction)\n",
    "else:\n",
    "    print(\"The new image is predicted to be non-cancerous.\")\n",
    "    print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 22ms/step\n",
      "The new image is predicted to be non-cancerous.\n",
      "0.0\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "The new image is predicted to be non-cancerous.\n",
      "0.0\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "The new image is predicted to be non-cancerous.\n",
      "0.0\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "The new image is predicted to be non-cancerous.\n",
      "0.0\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "The new image is predicted to be non-cancerous.\n",
      "0.0\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "The new image is predicted to be non-cancerous.\n",
      "0.0\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "The new image is predicted to be non-cancerous.\n",
      "0.0\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "The new image is predicted to be non-cancerous.\n",
      "0.0\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "The new image is predicted to be non-cancerous.\n",
      "0.0\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "The new image is predicted to be non-cancerous.\n",
      "0.0\n",
      "62/62 [==============================] - 2s 34ms/step - loss: 0.2491 - accuracy: 0.8995\n",
      "Validation loss: 0.249129980802536\n",
      "Validation accuracy: 0.8994949460029602\n",
      "248/248 [==============================] - 7s 29ms/step - loss: 0.2086 - accuracy: 0.9136\n",
      "Training loss: 0.20861981809139252\n",
      "Training accuracy: 0.9135926961898804\n"
     ]
    }
   ],
   "source": [
    "predict_extra_data(model)\n",
    "get_accuracy_and_loss(model,X_test,y_test,X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using grayscale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, Dropout\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "# Step 1: Read the CSV file and create a dataframe\n",
    "csv_file = \"data_labels_mainData.csv\"\n",
    "data = pd.read_csv(csv_file)\n",
    "\n",
    "# Step 2: Read the images from the directory and convert them to arrays\n",
    "image_dir = \"patch_images\"\n",
    "image_arrays = []\n",
    "for image_name in data[\"ImageName\"]:\n",
    "    image_path = os.path.join(image_dir, image_name)\n",
    "    img = cv2.imread(image_path)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) # convert to grayscale format\n",
    "    img = cv2.resize(img, (224, 224))\n",
    "    image_arrays.append(img)\n",
    "image_arrays = np.array(image_arrays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"image_data\"] = list(image_arrays)\n",
    "data = data.drop(\"ImageName\", axis=1)\n",
    "\n",
    "# Step 4: Split the data into training and testing sets\n",
    "X = data.drop(\"isCancerous\", axis=1)\n",
    "y = data[\"isCancerous\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Define the model\n",
    "def create_model(conv1_filters=32, conv2_filters=64, dense_units=128, dropout_rate=0.5):\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(conv1_filters, kernel_size=(3, 3), activation=\"relu\", input_shape=(224, 224, 1)))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Conv2D(conv2_filters, kernel_size=(3, 3), activation=\"relu\"))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(dense_units, activation=\"relu\"))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(1, activation=\"sigmoid\"))\n",
    "    model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "    return model\n",
    "\n",
    "\n",
    "# Step 6: Define the hyperparameters to tune and perform the search\n",
    "param_grid = {\n",
    "    \"conv1_filters\": [32, 64],\n",
    "    \"conv2_filters\": [64, 128],\n",
    "    \"dense_units\": [128, 256],\n",
    "    \"dropout_rate\": [0.3, 0.5],\n",
    "}\n",
    "deeper_model = KerasClassifier(\n",
    "    build_fn=create_model,\n",
    "    epochs=10,\n",
    "    batch_size=32,\n",
    "    verbose=2\n",
    ")\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=deeper_model,\n",
    "    param_grid=param_grid,\n",
    "    cv=3,\n",
    "    n_jobs=-1,\n",
    "    verbose=2,\n",
    "    scoring=\"accuracy\",\n",
    ")\n",
    "grid_search.fit(np.stack(X_train[\"image_data\"]), y_train)\n",
    "\n",
    "# Step 7: Print the best parameters and accuracy score\n",
    "print(\"Best parameters:\", grid_search.best_params_)\n",
    "print(\"Accuracy score:\", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 5: Make predictions on a new image\n",
    "new_image_path = \"patch_images/4850.png\"\n",
    "new_image = cv2.resize(cv2.imread(new_image_path, cv2.IMREAD_GRAYSCALE), (224, 224)) / 255.0\n",
    "new_image_array = np.array([new_image])\n",
    "prediction = grid_search.predict(new_image_array)\n",
    "if prediction[0] > 0.5:\n",
    "    print(\"The image is cancerous.\")\n",
    "else:\n",
    "    print(\"The image is not cancerous.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = grid_search.predict(np.stack(X_test[\"image_data\"]))\n",
    "y_pred = (y_pred > 0.5).astype(int)\n",
    "\n",
    "# Step 7: Evaluate the model's performance on the test data\n",
    "accuracy = (y_pred == y_test.values.reshape(-1,1)).mean()\n",
    "print(\"Accuracy on test set:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
